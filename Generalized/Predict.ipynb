{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predict.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOEDXBt/5VQ8BZXhxT4c5Fh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "db334cc7216f40e19dc727a531a1d01d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bfad73798e8a417f9e5be1962657cc9a",
              "IPY_MODEL_834379edaa0e4a608d9a159201ee8ccd",
              "IPY_MODEL_5348794845214cc68137d86830ca2857"
            ],
            "layout": "IPY_MODEL_1808546a49d84f7fbd59e97d04e0882b"
          }
        },
        "bfad73798e8a417f9e5be1962657cc9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9954fc5c9a504d6f960c65f99f793bc9",
            "placeholder": "​",
            "style": "IPY_MODEL_b799ed2f345645b4995ddb1097732aa8",
            "value": "100%"
          }
        },
        "834379edaa0e4a608d9a159201ee8ccd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1203fb1594e641e29dd968bd884de706",
            "max": 244408911,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a2a600dcd4b04077ac8be21a028bec1d",
            "value": 244408911
          }
        },
        "5348794845214cc68137d86830ca2857": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24a7b62cde03406f8a8e7bf01bd18433",
            "placeholder": "​",
            "style": "IPY_MODEL_475714d470364bfe934054a0f591d7aa",
            "value": " 233M/233M [00:05&lt;00:00, 74.6MB/s]"
          }
        },
        "1808546a49d84f7fbd59e97d04e0882b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9954fc5c9a504d6f960c65f99f793bc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b799ed2f345645b4995ddb1097732aa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1203fb1594e641e29dd968bd884de706": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2a600dcd4b04077ac8be21a028bec1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "24a7b62cde03406f8a8e7bf01bd18433": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "475714d470364bfe934054a0f591d7aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rdkworld/AIPND-2022/blob/main/Generalized/Predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Download image files\n",
        "!wget https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\n",
        "!tar -xf 102flowers.tgz"
      ],
      "metadata": {
        "id": "sqsx9QgdPNGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Clone Model Inference Repository\n",
        "!git lfs install\n",
        "!git clone https://rdkulkarni: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@huggingface.co/spaces/rdkulkarni/which-flower\n",
        "%cd which-flower\n",
        "!git init\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5T-lvlBdInrl",
        "outputId": "8abd83ba-0cd6-4b4b-fbf4-1256f3350b50"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated git hooks.\n",
            "Git LFS initialized.\n",
            "Cloning into 'which-flower'...\n",
            "remote: Enumerating objects: 235, done.\u001b[K\n",
            "remote: Counting objects: 100% (235/235), done.\u001b[K\n",
            "remote: Compressing objects: 100% (230/230), done.\u001b[K\n",
            "remote: Total 235 (delta 112), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (235/235), 1.11 MiB | 3.61 MiB/s, done.\n",
            "Resolving deltas: 100% (112/112), done.\n",
            "Filtering content: 100% (11/11), 1.58 GiB | 58.15 MiB/s, done.\n",
            "/content/which-flower\n",
            "Reinitialized existing Git repository in /content/which-flower/.git/\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from torchvision import models\n",
        "from torch import nn \n",
        "from typing import List"
      ],
      "metadata": {
        "id": "YNzExNIcQ7ul"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Update last layer of model\n",
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "def update_last_layer_pretrained_model(pretrained_model, num_classes, feature_extract):\n",
        "    set_parameter_requires_grad(pretrained_model, feature_extract)\n",
        "    if hasattr(pretrained_model, 'fc') and 'resnet' in pretrained_model.__class__.__name__.lower(): #resnet\n",
        "        num_ftrs = pretrained_model.fc.in_features\n",
        "        pretrained_model.fc = nn.Linear(num_ftrs, num_classes, bias = True)\n",
        "    elif hasattr(pretrained_model, 'classifier') and ('alexnet' in pretrained_model.__class__.__name__.lower() or 'vgg' in pretrained_model.__class__.__name__.lower()): #alexNet, vgg\n",
        "        num_ftrs = pretrained_model.classifier[6].in_features\n",
        "        pretrained_model.classifier[6] = nn.Linear(num_ftrs, num_classes, bias = True)\n",
        "    elif hasattr(pretrained_model, 'classifier') and 'squeezenet' in pretrained_model.__class__.__name__.lower(): #squeezenet\n",
        "        pretrained_model.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "        pretrained_model.num_classes = num_classes\n",
        "    elif hasattr(pretrained_model, 'classifier') and ('efficientnet' in pretrained_model.__class__.__name__.lower() or 'mobilenet' in pretrained_model.__class__.__name__.lower()): #efficientnet, mobilenet\n",
        "        num_ftrs = pretrained_model.classifier[1].in_features\n",
        "        pretrained_model.classifier[1] = nn.Linear(num_ftrs, num_classes, bias = True)\n",
        "    elif hasattr(pretrained_model, 'AuxLogits') and 'inception' in pretrained_model.__class__.__name__.lower(): #inception\n",
        "        num_ftrs = pretrained_model.AuxLogits.fc.in_features \n",
        "        pretrained_model.AuxLogits.fc = nn.Linear(num_ftrs, num_classes) #Auxilary net\n",
        "        num_ftrs = pretrained_model.fc.in_features\n",
        "        pretrained_model.fc = nn.Linear(num_ftrs,num_classes) #Primary net\n",
        "    elif hasattr(pretrained_model, 'classifier') and 'densenet' in pretrained_model.__class__.__name__.lower(): #densenet\n",
        "        num_ftrs = pretrained_model.classifier.in_features\n",
        "        pretrained_model.classifier = nn.Linear(num_ftrs, num_classes, bias = True)\n",
        "    elif hasattr(pretrained_model, 'heads') and 'visiontransformer' in pretrained_model.__class__.__name__.lower(): #vit transformer\n",
        "        num_ftrs = pretrained_model.heads.head.in_features\n",
        "        pretrained_model.heads.head = nn.Linear(num_ftrs, num_classes, bias = True)\n",
        "    elif hasattr(pretrained_model, 'head') and 'swin' in pretrained_model.__class__.__name__.lower(): #swin transformer\n",
        "        num_ftrs = pretrained_model.head.in_features\n",
        "        pretrained_model.head = nn.Linear(num_ftrs, num_classes, bias = True)\n",
        "    return pretrained_model"
      ],
      "metadata": {
        "id": "4tFkAYydROwc"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Model\n",
        "model_name, model_weights, model_path = ('alexnet','AlexNet_Weights','which-flower/flowers_alexnet_model.pth')\n",
        "checkpoint = torch.load(model_path, map_location='cpu')\n",
        "pretrained_weights = eval(f\"models.{model_weights}.DEFAULT\")\n",
        "auto_transforms = pretrained_weights.transforms()\n",
        "#pretrained_model = eval(f\"torchvision.models.{model_name}(weights = pretrained_weights)\")\n",
        "pretrained_model = eval(f\"models.{model_name}(pretrained = True)\")\n",
        "pretrained_model = update_last_layer_pretrained_model(pretrained_model, 102, True)\n",
        "pretrained_model.class_to_idx = checkpoint['class_to_idx']\n",
        "pretrained_model.class_names = checkpoint['class_names']    \n",
        "pretrained_model.load_state_dict(checkpoint['state_dict'])\n",
        "pretrained_model.to('cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650,
          "referenced_widgets": [
            "db334cc7216f40e19dc727a531a1d01d",
            "bfad73798e8a417f9e5be1962657cc9a",
            "834379edaa0e4a608d9a159201ee8ccd",
            "5348794845214cc68137d86830ca2857",
            "1808546a49d84f7fbd59e97d04e0882b",
            "9954fc5c9a504d6f960c65f99f793bc9",
            "b799ed2f345645b4995ddb1097732aa8",
            "1203fb1594e641e29dd968bd884de706",
            "a2a600dcd4b04077ac8be21a028bec1d",
            "24a7b62cde03406f8a8e7bf01bd18433",
            "475714d470364bfe934054a0f591d7aa"
          ]
        },
        "id": "Swx1UDIGQpiH",
        "outputId": "e7a6248b-30c1-4e57-fff8-eb42b5dd1235"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/233M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db334cc7216f40e19dc727a531a1d01d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlexNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=4096, out_features=102, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pred_and_plot_image\n",
        "def pred_and_plot_image(\n",
        "    model: torch.nn.Module,\n",
        "    image_path: str,\n",
        "    class_names: List[str] = None,\n",
        "    transform=None,\n",
        "    device: torch.device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "):\n",
        "\n",
        "    # 1. Load in image and convert the tensor values to float32\n",
        "    #target_image = torchvision.io.read_image(str(image_path)).type(torch.float32)\n",
        "    target_image = Image.open(image_path)\n",
        "\n",
        "    # 2. Divide the image pixel values by 255 to get them between [0, 1]\n",
        "    #target_image = target_image / 255.0\n",
        "\n",
        "    # 3. Transform if necessary\n",
        "    if transform:\n",
        "        target_image = transform(target_image)\n",
        "\n",
        "    # 4. Make sure the model is on the target device\n",
        "    model.to(device)\n",
        "\n",
        "    # 5. Turn on model evaluation mode and inference mode\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        # Add an extra dimension to the image\n",
        "        target_image = target_image.unsqueeze(dim=0)\n",
        "\n",
        "        # Make a prediction on image with an extra dimension and send it to the target device\n",
        "        target_image_pred = model(target_image.to(device))\n",
        "        print(target_image_pred)\n",
        "\n",
        "    # 6. Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n",
        "    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n",
        "    ps = target_image_pred_probs.topk(3)\n",
        "    #ps = target_image_pred_probs.max().cpu()    \n",
        "    # print(target_image_pred_probs)\n",
        "\n",
        "    # 7. Convert prediction probabilities -> prediction labels\n",
        "    #target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n",
        "    # print(target_image_pred_label)\n",
        "\n",
        "    # if class_names:\n",
        "    #     idxs= class_names[target_image_pred_label.cpu()]\n",
        "    # else:\n",
        "    #     idxs = target_image_pred_label\n",
        "    \n",
        "\n",
        "    print(ps)\n",
        "    print(idxs)\n",
        "\n",
        "    return (ps) #idxs)"
      ],
      "metadata": {
        "id": "9jY8ohaSUFhl"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict\n",
        "image_path = 'which-flower/16_image_06670.jpg'\n",
        "probs = pred_and_plot_image(model=pretrained_model, image_path=image_path, class_names=pretrained_model.class_names, transform=auto_transforms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mriTN8blQSuz",
        "outputId": "5bbacf1a-5c58-45b2-fb0e-28aca3bc5a9c"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-131.0154, -104.4875, -111.4227,  -69.5271, -129.0562,  -64.2941,\n",
            "          -62.7256,  -90.4476,  -92.9090,  -41.9443,  -15.8085,  -37.0751,\n",
            "          -80.9203, -109.7665,  -64.3132,  -48.8316,  -59.6381,  -70.8267,\n",
            "          -94.4491,  -97.3581,  -70.7337,  -84.3126,  -97.3874,  -87.5583,\n",
            "          -70.5540,  -70.2662, -115.3786,  -79.2373, -103.0447, -101.9497,\n",
            "          -79.1343,  -91.7792,  -94.2285, -165.4154, -141.8475, -101.9314,\n",
            "          -90.8153,  -52.6507,  -90.2181,  -86.7901,  -47.1341, -155.1535,\n",
            "          -80.0814,  -54.4336,  -78.7549,  -67.8433,  -58.1513,  -49.5928,\n",
            "         -102.1684,  -87.1291,  -75.4091,  -86.5833,  -40.9567,  -99.1294,\n",
            "         -100.4106, -113.2578,  -93.5447,  -80.2042,  -54.7489, -110.0415,\n",
            "         -120.8002,  -69.8167,  -97.2944, -104.0411,  -51.6064, -117.8205,\n",
            "          -45.4143,  -66.2321, -104.5364,  -86.3309,  -29.6400,  -84.6164,\n",
            "          -81.9052,  -81.6152,  -34.2829, -116.8071,  -85.4303, -103.6517,\n",
            "          -90.9032,  -84.4038,  -95.0529, -115.2203,  -84.4601, -101.0119,\n",
            "          -39.8579,  -85.4190, -111.0597, -113.2572, -114.5677,  -70.2186,\n",
            "          -45.2859, -105.9223,  -43.0789, -104.2419, -129.8193,  -89.1915,\n",
            "          -70.6057,  -50.4655,  -52.6077,   -6.5962,  -83.6799, -119.0175]])\n",
            "torch.return_types.topk(\n",
            "values=tensor([[9.9990e-01, 9.9786e-05, 9.8209e-11]]),\n",
            "indices=tensor([[99, 10, 70]]))\n",
            "97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!gzip -d flower-models-repo/flowers.tar.gz\n",
        "#!rm -rf which-flower/\n",
        "auto_transforms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCDJ8vHeYpTK",
        "outputId": "83f74e1b-ae18-4881-9d07-09c8a6492d1f"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ImageClassification(\n",
              "    crop_size=[224]\n",
              "    resize_size=[256]\n",
              "    mean=[0.485, 0.456, 0.406]\n",
              "    std=[0.229, 0.224, 0.225]\n",
              "    interpolation=InterpolationMode.BILINEAR\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    }
  ]
}