{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train an Existing Pytorch Model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNIYssvLoJ4nbjHOK5P/ti0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rdkworld/AIPND-2022/blob/main/Generalized/Train_an_Existing_Pytorch_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pre-requisite Setup"
      ],
      "metadata": {
        "id": "0PB5a4bgZY0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Get Libraries"
      ],
      "metadata": {
        "id": "06WelAjucNyF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7zy5B3MY-ON",
        "outputId": "98d82fbe-3b9f-4ed9-d614-bc51e8e0e51f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 1.12.1+cu113\n",
            "torchvision version: 0.13.1+cu113\n"
          ]
        }
      ],
      "source": [
        "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
        "try:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
        "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")\n",
        "except:\n",
        "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
        "    !pip3 install -U --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cu113\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Regular Imports"
      ],
      "metadata": {
        "id": "TjUjLP0OcUQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue with regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "#Additions from functions\n",
        "import os\n",
        "import tarfile"
      ],
      "metadata": {
        "id": "jxBuV0LRalcM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Helpers/functions from Github"
      ],
      "metadata": {
        "id": "FvxZuuRJcieX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try to import the helper functions, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
        "except:\n",
        "    # Get the scripts\n",
        "    print(\"[INFO] Couldn't find the scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/rdkworld/AIPND-2022\n",
        "    !mv AIPND-2022/Generalized/*.py .\n",
        "    !rm -rf AIPND-2022\n",
        "    import data_setup, engine, model_builder, utils \n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves"
      ],
      "metadata": {
        "id": "xy7UmQPTchnY"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[INFO] Couldn't find the scripts... downloading them from GitHub.\")\n",
        "!git clone https://github.com/rdkworld/AIPND-2022\n",
        "!mv AIPND-2022/Generalized/*.py .\n",
        "!rm -rf AIPND-2022\n",
        "import data_setup, engine, model_builder, utils \n",
        "from helper_functions import download_data, set_seeds, plot_loss_curves"
      ],
      "metadata": {
        "id": "7lqeVE4iJ58B",
        "outputId": "bde9d9c7-5e90-4fff-ed32-53925e75ded2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Couldn't find the scripts... downloading them from GitHub.\n",
            "Cloning into 'AIPND-2022'...\n",
            "remote: Enumerating objects: 304, done.\u001b[K\n",
            "remote: Counting objects: 100% (194/194), done.\u001b[K\n",
            "remote: Compressing objects: 100% (138/138), done.\u001b[K\n",
            "remote: Total 304 (delta 98), reused 116 (delta 53), pack-reused 110\u001b[K\n",
            "Receiving objects: 100% (304/304), 11.00 MiB | 41.27 MiB/s, done.\n",
            "Resolving deltas: 100% (120/120), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connect Colab and Google Drive to save and load models"
      ],
      "metadata": {
        "id": "hpHEp3Wsg5Nn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount Google Drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-8ByoAzAg3sB",
        "outputId": "b30220a3-9885-4258-ff3c-c503bb91eff7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Setup target device"
      ],
      "metadata": {
        "id": "ObsXXY47efcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup target device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "P61CnE2WedzB",
        "outputId": "f8827d6f-758f-413e-d880-e2ad3a34e6c6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## User Input Parameters including Hyperparameters"
      ],
      "metadata": {
        "id": "h_MFjg_Ke8G1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Data\n",
        "SOURCE_URL = 'https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz'\n",
        "BASE_DATA_DIRECTORY = 'data'\n",
        "PROJECT_DATA_DIRECTORY = 'flowers'\n",
        "FILE_NAME = 'flowers.tar.gz'\n",
        "train_dir = f\"{BASE_DATA_DIRECTORY}/{PROJECT_DATA_DIRECTORY}/train\"\n",
        "valid_dir = f\"{BASE_DATA_DIRECTORY}/{PROJECT_DATA_DIRECTORY}/valid\"\n",
        "test_dir = f\"{BASE_DATA_DIRECTORY}/{PROJECT_DATA_DIRECTORY}/test\"\n",
        "\n",
        "# Setup hyperparameters\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 64\n",
        "HIDDEN_UNITS = '' #Not used\n",
        "LEARNING_RATE = 0.003\n",
        "MODEL_NAME = 'vit_b_16'\n",
        "MODEL_WEIGHT = 'ViT_B_16' \n",
        "LOSS_FUNCTION = 'CrossEntropyLoss'\n",
        "OPTIMIZER = 'Adam'\n",
        "MANUAL_RESIZE = 64\n",
        "NUM_CLASSES = 102\n",
        "FEATURE_EXTRACT = True\n",
        "RGB = 3 #(Color picture is 3, black & white is 1) \n"
      ],
      "metadata": {
        "id": "5Eo7rXBefwJT"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Download data and categorize into train/valid/test folders as required"
      ],
      "metadata": {
        "id": "IFcdIJxzdetT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Work in Progress, See next cell\n",
        "!ls $BASE_DATA_DIRECTORY/\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_uuT5PTdAgF",
        "outputId": "f72bbaf1-9d33-41f0-bdaa-bb4c110748bd"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "flowers\n",
            "'$BASE_DATA_DIRECTORY'\t drive\t\t       model_builder.py   sample_data\n",
            " data\t\t\t engine.py\t       predictions.py\t  train.py\n",
            " data_setup.py\t\t helper_functions.py   __pycache__\t  utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Temporarily copy from Google Drive\n",
        "!mkdir $BASE_DATA_DIRECTORY\n",
        "!cp /content/drive/MyDrive/flowers.tar.gz data/ #copy from drive to colab"
      ],
      "metadata": {
        "id": "HIo3yrwvh8mM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Untar the file\n",
        "with tarfile.open(os.path.join(BASE_DATA_DIRECTORY, FILE_NAME), \"r\") as tar_ref:\n",
        "    print(f\"[INFO] Unzipping {FILE_NAME}...\") \n",
        "    tar_ref.extractall(BASE_DATA_DIRECTORY)"
      ],
      "metadata": {
        "id": "omb0fWoWn49x",
        "outputId": "bfc68bbc-bb3a-47e9-bd9c-9b1d669fe5b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-3d19190c6af5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Untar the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_DATA_DIRECTORY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFILE_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtar_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[INFO] Unzipping {FILE_NAME}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtar_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_DATA_DIRECTORY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/tarfile.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(cls, name, mode, fileobj, bufsize, **kwargs)\u001b[0m\n\u001b[1;32m   1573\u001b[0m                     \u001b[0msaved_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1574\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1575\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1576\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mReadError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompressionError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1577\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/tarfile.py\u001b[0m in \u001b[0;36mgzopen\u001b[0;34m(cls, name, mode, fileobj, compresslevel, **kwargs)\u001b[0m\n\u001b[1;32m   1637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1639\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1640\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/flowers.tar.gz'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!tar xf $BASE_DATA_DIRECTORY/$FILE_NAME"
      ],
      "metadata": {
        "id": "r4N5Dm10q_u5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(os.path.join(BASE_DATA_DIRECTORY, PROJECT_DATA_DIRECTORY)):\n",
        "  os.remove(os.path.join(BASE_DATA_DIRECTORY, FILE_NAME))"
      ],
      "metadata": {
        "id": "a0L2o97mouw4"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get info on Pre-Trained Models"
      ],
      "metadata": {
        "id": "gevSxhRFxrVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Pre-trained Model & Transform Details"
      ],
      "metadata": {
        "id": "6hUgXVUs6Rgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get pre-trained model weights and model\n",
        "pretrained_weights = eval(f\"torchvision.models.{MODEL_WEIGHT}_Weights.DEFAULT\")\n",
        "pretrained_model = eval(f\"torchvision.models.{MODEL_NAME}(weights = pretrained_weights)\").to(device)\n",
        "auto_transforms = pretrained_weights.transforms()"
      ],
      "metadata": {
        "id": "JQdS88CIu6fn"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a summary using torchinfo (uncomment for actual output)\n",
        "summary(model=pretrained_model,\n",
        "        input_size= (BATCH_SIZE, RGB, auto_transforms.crop_size[0], auto_transforms.crop_size[0]),  # make sure this is \"input_size\", not \"input_shape\"\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],  # col_names=[\"input_size\"], # uncomment for smaller output\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"]\n",
        "), auto_transforms"
      ],
      "metadata": {
        "id": "dfvSvX7f7TBY",
        "outputId": "b3bdd869-c4ea-47d7-ef95-5aa28b650a05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(======================================================================================================================================================\n",
              " Layer (type (var_name))                                                Input Shape          Output Shape         Param #              Trainable\n",
              " ======================================================================================================================================================\n",
              " VisionTransformer (VisionTransformer)                                  [64, 3, 224, 224]    [64, 1000]           768                  True\n",
              " ├─Conv2d (conv_proj)                                                   [64, 3, 224, 224]    [64, 768, 14, 14]    590,592              True\n",
              " ├─Encoder (encoder)                                                    [64, 197, 768]       [64, 197, 768]       151,296              True\n",
              " │    └─Dropout (dropout)                                               [64, 197, 768]       [64, 197, 768]       --                   --\n",
              " │    └─Sequential (layers)                                             [64, 197, 768]       [64, 197, 768]       --                   True\n",
              " │    │    └─EncoderBlock (encoder_layer_0)                             [64, 197, 768]       [64, 197, 768]       7,087,872            True\n",
              " │    │    └─EncoderBlock (encoder_layer_1)                             [64, 197, 768]       [64, 197, 768]       7,087,872            True\n",
              " │    │    └─EncoderBlock (encoder_layer_2)                             [64, 197, 768]       [64, 197, 768]       7,087,872            True\n",
              " │    │    └─EncoderBlock (encoder_layer_3)                             [64, 197, 768]       [64, 197, 768]       7,087,872            True\n",
              " │    │    └─EncoderBlock (encoder_layer_4)                             [64, 197, 768]       [64, 197, 768]       7,087,872            True\n",
              " │    │    └─EncoderBlock (encoder_layer_5)                             [64, 197, 768]       [64, 197, 768]       7,087,872            True\n",
              " │    │    └─EncoderBlock (encoder_layer_6)                             [64, 197, 768]       [64, 197, 768]       7,087,872            True\n",
              " │    │    └─EncoderBlock (encoder_layer_7)                             [64, 197, 768]       [64, 197, 768]       7,087,872            True\n",
              " │    │    └─EncoderBlock (encoder_layer_8)                             [64, 197, 768]       [64, 197, 768]       7,087,872            True\n",
              " │    │    └─EncoderBlock (encoder_layer_9)                             [64, 197, 768]       [64, 197, 768]       7,087,872            True\n",
              " │    │    └─EncoderBlock (encoder_layer_10)                            [64, 197, 768]       [64, 197, 768]       7,087,872            True\n",
              " │    │    └─EncoderBlock (encoder_layer_11)                            [64, 197, 768]       [64, 197, 768]       7,087,872            True\n",
              " │    └─LayerNorm (ln)                                                  [64, 197, 768]       [64, 197, 768]       1,536                True\n",
              " ├─Sequential (heads)                                                   [64, 768]            [64, 1000]           --                   True\n",
              " │    └─Linear (head)                                                   [64, 768]            [64, 1000]           769,000              True\n",
              " ======================================================================================================================================================\n",
              " Total params: 86,567,656\n",
              " Trainable params: 86,567,656\n",
              " Non-trainable params: 0\n",
              " Total mult-adds (G): 11.09\n",
              " ======================================================================================================================================================\n",
              " Input size (MB): 38.54\n",
              " Forward/backward pass size (MB): 6661.98\n",
              " Params size (MB): 260.62\n",
              " Estimated Total Size (MB): 6961.14\n",
              " ======================================================================================================================================================,\n",
              " ImageClassification(\n",
              "     crop_size=[224]\n",
              "     resize_size=[256]\n",
              "     mean=[0.485, 0.456, 0.406]\n",
              "     std=[0.229, 0.224, 0.225]\n",
              "     interpolation=InterpolationMode.BILINEAR\n",
              " ))"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat model_builder.py"
      ],
      "metadata": {
        "id": "MBRDb_-iKIb5",
        "outputId": "d52d724a-3555-4521-ce99-7ce93dc5f231",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"\"\"\n",
            "Contains PyTorch model code to instantiate a TinyVGG model.\n",
            "\"\"\"\n",
            "import torch\n",
            "from torch import nn \n",
            "from torchvision import models\n",
            "from torch import nn\n",
            "from collections import OrderedDict\n",
            "\n",
            "def set_parameter_requires_grad(model, feature_extracting):\n",
            "    if feature_extracting:\n",
            "        for param in model.parameters():\n",
            "            param.requires_grad = False\n",
            "\n",
            "def update_last_layer_pretrained_model(pretrained_model, num_classes, feature_extract):\n",
            "    set_parameter_requires_grad(pretrained_model, feature_extract)\n",
            "    if getattr(pretrained_model, 'heads'):\n",
            "        num_ftrs = pretrained_model.heads.head.in_features\n",
            "        pretrained_model.heads.head = nn.Linear(num_ftrs, num_classes, bias = True)\n",
            "    return pretrained_model\n",
            "\n",
            "def initialize_existing_models(model_name, model_type, num_classes, feature_extract, hidden_units, use_pretrained=True):\n",
            "    # Initialize these variables which will be set in this if statement. Each of these variables is model specific.\n",
            "    model_ft = None\n",
            "    input_size = 0\n",
            "\n",
            "    if model_name == \"resnet18\":\n",
            "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
            "        set_parameter_requires_grad(model_ft, feature_extract)\n",
            "        num_ftrs = model_ft.fc.in_features\n",
            "        model_ft.fc = nn.Sequential(\n",
            "                        nn.Linear(num_ftrs, num_classes),\n",
            "                        nn.LogSoftmax(dim=1))\n",
            "        input_size = 224\n",
            "    elif model_name == \"alexnet\":\n",
            "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
            "        set_parameter_requires_grad(model_ft, feature_extract)\n",
            "        num_ftrs = model_ft.classifier[6].in_features\n",
            "        model_ft.classifier[6] = nn.Sequential(\n",
            "                                    nn.Linear(num_ftrs,num_classes),\n",
            "                                    nn.LogSoftmax(dim=1))        \n",
            "        input_size = 224\n",
            "    elif model_name in [\"vgg11_bn\", \"vgg13\", \"vgg16\"]:\n",
            "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
            "        set_parameter_requires_grad(model_ft, feature_extract)\n",
            "        num_ftrs = model_ft.classifier[6].in_features\n",
            "        model_ft.classifier[6] = nn.Sequential(\n",
            "                                    nn.Linear(num_ftrs,num_classes),\n",
            "                                    nn.LogSoftmax(dim=1))\n",
            "        input_size = 224\n",
            "    elif model_name == \"squeezenet\":\n",
            "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
            "        set_parameter_requires_grad(model_ft, feature_extract)\n",
            "        model_ft.classifier[1] = nn.Sequential(\n",
            "                                    nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1)),\n",
            "                                    nn.LogSoftmax(dim=1))\n",
            "        model_ft.num_classes = num_classes\n",
            "        input_size = 224\n",
            "    elif model_name == \"densenet121\":\n",
            "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
            "        set_parameter_requires_grad(model_ft, feature_extract)\n",
            "        num_ftrs = model_ft.classifier.in_features\n",
            "        model_ft.classifier = nn.Sequential(\n",
            "                                    nn.Linear(num_ftrs, num_classes),\n",
            "                                    nn.LogSoftmax(dim=1))     \n",
            "        input_size = 224\n",
            "    elif model_name == \"inception\": # This model expects (299,299) sized images and has auxiliary output\n",
            "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
            "        set_parameter_requires_grad(model_ft, feature_extract)\n",
            "        # Handle the auxilary net\n",
            "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
            "        model_ft.AuxLogits.fc = nn.Sequential(\n",
            "                                    nn.Linear(num_ftrs, num_classes),\n",
            "                                    nn.LogSoftmax(dim=1))\n",
            "        # Handle the primary net\n",
            "        num_ftrs = model_ft.fc.in_features\n",
            "        model_ft.fc = nn.Sequential(\n",
            "                            nn.Linear(num_ftrs,num_classes),\n",
            "                            nn.LogSoftmax(dim=1))\n",
            "        input_size = 299\n",
            "    else:\n",
            "        print(\"Invalid model name, please use one of the models supported by this application, exiting...\")\n",
            "        exit()\n",
            "    return model_ft, input_size\n",
            "\n",
            "#Get pre-trained model specifications and override with classifier portion with user activation units\n",
            "def build_custom_models(model_name, model_type, num_classes, feature_extract, hidden_units, use_pretrained=True):\n",
            "       \n",
            "    model_ft = getattr(models, model_name)(pretrained = use_pretrained)\n",
            "    set_parameter_requires_grad(model_ft, feature_extract)\n",
            "    if model_name == 'resnet18':\n",
            "        in_features = model_ft.fc.in_features\n",
            "    else:\n",
            "        try: #Is there an iterable classifier layer for the model chosen?\n",
            "            iter(model_ft.classifier)\n",
            "        except TypeError: #If no, choose the classifier layer with no index\n",
            "            in_features = model_ft.classifier.in_features\n",
            "        else:\n",
            "            try: #If yes, check if first index has in_features attribute\n",
            "                in_features = model_ft.classifier[0].in_features\n",
            "            except AttributeError: #If No, check if second index has in_features attribute\n",
            "                in_features = model_ft.classifier[1].in_features\n",
            "        \n",
            "    hidden_layers = [in_features] + hidden_units\n",
            "    layer_builder = (\n",
            "        lambda i, v : (f\"fc{i}\", nn.Linear(hidden_layers[i-1], v)),\n",
            "        lambda i, v: (f\"relu{i}\", nn.ReLU()),\n",
            "        lambda i, v: (f\"drop{i}\", nn.Dropout())        \n",
            "    )\n",
            "    \n",
            "    layers = [f(i, v) for i, v in enumerate(hidden_layers) if i > 0 for f in layer_builder]\n",
            "    layers += [('fc_final', nn.Linear(hidden_layers[-1], num_classes)),\n",
            "               ('output', nn.LogSoftmax(dim=1))]    \n",
            "\n",
            "    if model_name == 'resnet18':\n",
            "        fc = nn.Sequential(OrderedDict(layers))\n",
            "        model_ft.fc = fc\n",
            "    else:\n",
            "        classifier = nn.Sequential(OrderedDict(layers))\n",
            "        model_ft.classifier = classifier\n",
            "#     print(\"AFTER\")\n",
            "#     print(model.classifier)\n",
            "    \n",
            "    return model_ft\n",
            "\n",
            "#Define model/ neural network class\n",
            "# class ImageClassifier(nn.Module):\n",
            "#     def __init__(self):\n",
            "#         super(ImageClassifer, self).__init__()\n",
            "#         self.flatten = nn.Flatten()\n",
            "#         self.model_stack = nn.Sequential(\n",
            "#             nn.Linear(),\n",
            "#             nn.ReLU(),\n",
            "#             nn.Dropout(0.2),\n",
            "#             nn.Linear(),\n",
            "#             nn.LogSoftmax(dim=1)\n",
            "#         )\n",
            "               \n",
            "#     def forward(self, x):\n",
            "#         x = self.flatten(x)\n",
            "#         logits = self.model_stack(x)\n",
            "#         return logits\n",
            "class TinyVGG(nn.Module):\n",
            "    \"\"\"Creates the TinyVGG architecture.\n",
            "\n",
            "    Replicates the TinyVGG architecture from the CNN explainer website in PyTorch.\n",
            "    See the original architecture here: https://poloclub.github.io/cnn-explainer/\n",
            "\n",
            "    Args:\n",
            "    input_shape: An integer indicating number of input channels.\n",
            "    hidden_units: An integer indicating number of hidden units between layers.\n",
            "    output_shape: An integer indicating number of output units.\n",
            "    \"\"\"\n",
            "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
            "        super().__init__()\n",
            "        self.conv_block_1 = nn.Sequential(\n",
            "          nn.Conv2d(in_channels=input_shape, \n",
            "                    out_channels=hidden_units, \n",
            "                    kernel_size=3, \n",
            "                    stride=1, \n",
            "                    padding=0),  \n",
            "          nn.ReLU(),\n",
            "          nn.Conv2d(in_channels=hidden_units, \n",
            "                    out_channels=hidden_units,\n",
            "                    kernel_size=3,\n",
            "                    stride=1,\n",
            "                    padding=0),\n",
            "          nn.ReLU(),\n",
            "          nn.MaxPool2d(kernel_size=2,\n",
            "                        stride=2)\n",
            "        )\n",
            "        self.conv_block_2 = nn.Sequential(\n",
            "          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
            "          nn.ReLU(),\n",
            "          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
            "          nn.ReLU(),\n",
            "          nn.MaxPool2d(2)\n",
            "        )\n",
            "        self.classifier = nn.Sequential(\n",
            "          nn.Flatten(),\n",
            "          # Where did this in_features shape come from? \n",
            "          # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
            "          nn.Linear(in_features=hidden_units*13*13,\n",
            "                    out_features=output_shape)\n",
            "        )\n",
            "    \n",
            "    def forward(self, x: torch.Tensor):\n",
            "        x = self.conv_block_1(x)\n",
            "        x = self.conv_block_2(x)\n",
            "        x = self.classifier(x)\n",
            "        return x\n",
            "        # return self.classifier(self.block_2(self.block_1(x))) # <- leverage the benefits of operator fusion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from model_builder import update_last_layer_pretrained_model"
      ],
      "metadata": {
        "id": "fLEK6EA1NuGY",
        "outputId": "2b8aed03-ba3d-4317-d4ae-9b2211eebc33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        }
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-146-3b65e1783cdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel_builder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mupdate_last_layer_pretrained_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'update_last_layer_pretrained_model' from 'model_builder' (/content/model_builder.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IqC2ZVXCOhlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "jn0DphgjOh8s",
        "outputId": "1dbc1481-afcb-463e-87a4-c2f1e2d5d03f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'$BASE_DATA_DIRECTORY'\t drive\t\t       model_builder.py   sample_data\n",
            " data\t\t\t engine.py\t       predictions.py\t  train.py\n",
            " data_setup.py\t\t helper_functions.py   __pycache__\t  utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model with help from model_builder.py\n",
        "updated_pretrained_model = model_builder.update_last_layer_pretrained_model(pretrained_model, NUM_CLASSES, FEATURE_EXTRACT).to(device)\n",
        "#updated_pretrained_model = update_last_layer_pretrained_model(pretrained_model, NUM_CLASSES, FEATURE_EXTRACT)"
      ],
      "metadata": {
        "id": "aU501-21IuyY",
        "outputId": "bbbe0cb4-c4aa-4bd9-adeb-e1eb301de568",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-145-02b87cfba1b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create model with help from model_builder.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mupdated_pretrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_last_layer_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFEATURE_EXTRACT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#updated_pretrained_model = update_last_layer_pretrained_model(pretrained_model, NUM_CLASSES, FEATURE_EXTRACT)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'model_builder' has no attribute 'update_last_layer_pretrained_model'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model=updated_pretrained_model,\n",
        "        input_size= (BATCH_SIZE, RGB, auto_transforms.crop_size[0], auto_transforms.crop_size[0]),  # make sure this is \"input_size\", not \"input_shape\"\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],  # col_names=[\"input_size\"], # uncomment for smaller output\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"]\n",
        ")"
      ],
      "metadata": {
        "id": "ZFOSJ8VvJptq",
        "outputId": "bbfbadaf-a1d6-4e2b-9801-1bd20d44dd8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "======================================================================================================================================================\n",
              "Layer (type (var_name))                                                Input Shape          Output Shape         Param #              Trainable\n",
              "======================================================================================================================================================\n",
              "VisionTransformer (VisionTransformer)                                  [64, 3, 224, 224]    [64, 102]            768                  Partial\n",
              "├─Conv2d (conv_proj)                                                   [64, 3, 224, 224]    [64, 768, 14, 14]    (590,592)            False\n",
              "├─Encoder (encoder)                                                    [64, 197, 768]       [64, 197, 768]       151,296              False\n",
              "│    └─Dropout (dropout)                                               [64, 197, 768]       [64, 197, 768]       --                   --\n",
              "│    └─Sequential (layers)                                             [64, 197, 768]       [64, 197, 768]       --                   False\n",
              "│    │    └─EncoderBlock (encoder_layer_0)                             [64, 197, 768]       [64, 197, 768]       (7,087,872)          False\n",
              "│    │    └─EncoderBlock (encoder_layer_1)                             [64, 197, 768]       [64, 197, 768]       (7,087,872)          False\n",
              "│    │    └─EncoderBlock (encoder_layer_2)                             [64, 197, 768]       [64, 197, 768]       (7,087,872)          False\n",
              "│    │    └─EncoderBlock (encoder_layer_3)                             [64, 197, 768]       [64, 197, 768]       (7,087,872)          False\n",
              "│    │    └─EncoderBlock (encoder_layer_4)                             [64, 197, 768]       [64, 197, 768]       (7,087,872)          False\n",
              "│    │    └─EncoderBlock (encoder_layer_5)                             [64, 197, 768]       [64, 197, 768]       (7,087,872)          False\n",
              "│    │    └─EncoderBlock (encoder_layer_6)                             [64, 197, 768]       [64, 197, 768]       (7,087,872)          False\n",
              "│    │    └─EncoderBlock (encoder_layer_7)                             [64, 197, 768]       [64, 197, 768]       (7,087,872)          False\n",
              "│    │    └─EncoderBlock (encoder_layer_8)                             [64, 197, 768]       [64, 197, 768]       (7,087,872)          False\n",
              "│    │    └─EncoderBlock (encoder_layer_9)                             [64, 197, 768]       [64, 197, 768]       (7,087,872)          False\n",
              "│    │    └─EncoderBlock (encoder_layer_10)                            [64, 197, 768]       [64, 197, 768]       (7,087,872)          False\n",
              "│    │    └─EncoderBlock (encoder_layer_11)                            [64, 197, 768]       [64, 197, 768]       (7,087,872)          False\n",
              "│    └─LayerNorm (ln)                                                  [64, 197, 768]       [64, 197, 768]       (1,536)              False\n",
              "├─Sequential (heads)                                                   [64, 768]            [64, 102]            --                   True\n",
              "│    └─Linear (head)                                                   [64, 768]            [64, 102]            78,438               True\n",
              "======================================================================================================================================================\n",
              "Total params: 85,877,094\n",
              "Trainable params: 78,438\n",
              "Non-trainable params: 85,798,656\n",
              "Total mult-adds (G): 11.04\n",
              "======================================================================================================================================================\n",
              "Input size (MB): 38.54\n",
              "Forward/backward pass size (MB): 6661.52\n",
              "Params size (MB): 257.85\n",
              "Estimated Total Size (MB): 6957.91\n",
              "======================================================================================================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.lib import pretty\n",
        "if getattr(pretrained_model, 'heads'):\n",
        "  print(True)\n",
        "pretrained_model.heads.head"
      ],
      "metadata": {
        "id": "zt89xVKI7ubz",
        "outputId": "6a62ebc6-3ecb-42be-ca9b-585adbdf6c7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=768, out_features=1000, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dataloaders"
      ],
      "metadata": {
        "id": "Sy0RxYxp5c-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=auto_transforms,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "if len(class_names) != NUM_CLASSES:\n",
        "  print(\"Mismatch in the number of unique classes/labels and user input NUM_CLASSES\")\n",
        "  exit()"
      ],
      "metadata": {
        "id": "dqJxC0I85gzz"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_builder.update_last_layer_pretrained_model"
      ],
      "metadata": {
        "id": "RgpF0MJP6yFK",
        "outputId": "f7655323-e9f2-4c72-b147-9d288ab53ae3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('data/flowers/train', 'data/flowers/test')"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Contains PyTorch model code to instantiate a TinyVGG model.\n",
        "\"\"\"\n",
        "import torch\n",
        "from torch import nn \n",
        "from torchvision import models\n",
        "from torch import nn\n",
        "from collections import OrderedDict\n",
        "\n",
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "def update_last_layer_pretrained_model(pretrained_model, num_classes, feature_extract):\n",
        "    set_parameter_requires_grad(pretrained_model, feature_extract)\n",
        "    if getattr(pretrained_model, 'heads'):\n",
        "        num_ftrs = pretrained_model.heads.head.in_features\n",
        "        pretrained_model.heads.head = nn.Linear(num_ftrs, num_classes, bias = True)\n",
        "    return pretrained_model\n",
        "\n",
        "def initialize_existing_models(model_name, model_type, num_classes, feature_extract, hidden_units, use_pretrained=True):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these variables is model specific.\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    if model_name == \"resnet18\":\n",
        "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Sequential(\n",
        "                        nn.Linear(num_ftrs, num_classes),\n",
        "                        nn.LogSoftmax(dim=1))\n",
        "        input_size = 224\n",
        "    elif model_name == \"alexnet\":\n",
        "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Sequential(\n",
        "                                    nn.Linear(num_ftrs,num_classes),\n",
        "                                    nn.LogSoftmax(dim=1))        \n",
        "        input_size = 224\n",
        "    elif model_name in [\"vgg11_bn\", \"vgg13\", \"vgg16\"]:\n",
        "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Sequential(\n",
        "                                    nn.Linear(num_ftrs,num_classes),\n",
        "                                    nn.LogSoftmax(dim=1))\n",
        "        input_size = 224\n",
        "    elif model_name == \"squeezenet\":\n",
        "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        model_ft.classifier[1] = nn.Sequential(\n",
        "                                    nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1)),\n",
        "                                    nn.LogSoftmax(dim=1))\n",
        "        model_ft.num_classes = num_classes\n",
        "        input_size = 224\n",
        "    elif model_name == \"densenet121\":\n",
        "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier.in_features\n",
        "        model_ft.classifier = nn.Sequential(\n",
        "                                    nn.Linear(num_ftrs, num_classes),\n",
        "                                    nn.LogSoftmax(dim=1))     \n",
        "        input_size = 224\n",
        "    elif model_name == \"inception\": # This model expects (299,299) sized images and has auxiliary output\n",
        "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        # Handle the auxilary net\n",
        "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
        "        model_ft.AuxLogits.fc = nn.Sequential(\n",
        "                                    nn.Linear(num_ftrs, num_classes),\n",
        "                                    nn.LogSoftmax(dim=1))\n",
        "        # Handle the primary net\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Sequential(\n",
        "                            nn.Linear(num_ftrs,num_classes),\n",
        "                            nn.LogSoftmax(dim=1))\n",
        "        input_size = 299\n",
        "    else:\n",
        "        print(\"Invalid model name, please use one of the models supported by this application, exiting...\")\n",
        "        exit()\n",
        "    return model_ft, input_size\n",
        "\n",
        "#Get pre-trained model specifications and override with classifier portion with user activation units\n",
        "def build_custom_models(model_name, model_type, num_classes, feature_extract, hidden_units, use_pretrained=True):\n",
        "       \n",
        "    model_ft = getattr(models, model_name)(pretrained = use_pretrained)\n",
        "    set_parameter_requires_grad(model_ft, feature_extract)\n",
        "    if model_name == 'resnet18':\n",
        "        in_features = model_ft.fc.in_features\n",
        "    else:\n",
        "        try: #Is there an iterable classifier layer for the model chosen?\n",
        "            iter(model_ft.classifier)\n",
        "        except TypeError: #If no, choose the classifier layer with no index\n",
        "            in_features = model_ft.classifier.in_features\n",
        "        else:\n",
        "            try: #If yes, check if first index has in_features attribute\n",
        "                in_features = model_ft.classifier[0].in_features\n",
        "            except AttributeError: #If No, check if second index has in_features attribute\n",
        "                in_features = model_ft.classifier[1].in_features\n",
        "        \n",
        "    hidden_layers = [in_features] + hidden_units\n",
        "    layer_builder = (\n",
        "        lambda i, v : (f\"fc{i}\", nn.Linear(hidden_layers[i-1], v)),\n",
        "        lambda i, v: (f\"relu{i}\", nn.ReLU()),\n",
        "        lambda i, v: (f\"drop{i}\", nn.Dropout())        \n",
        "    )\n",
        "    \n",
        "    layers = [f(i, v) for i, v in enumerate(hidden_layers) if i > 0 for f in layer_builder]\n",
        "    layers += [('fc_final', nn.Linear(hidden_layers[-1], num_classes)),\n",
        "               ('output', nn.LogSoftmax(dim=1))]    \n",
        "\n",
        "    if model_name == 'resnet18':\n",
        "        fc = nn.Sequential(OrderedDict(layers))\n",
        "        model_ft.fc = fc\n",
        "    else:\n",
        "        classifier = nn.Sequential(OrderedDict(layers))\n",
        "        model_ft.classifier = classifier\n",
        "#     print(\"AFTER\")\n",
        "#     print(model.classifier)\n",
        "    \n",
        "    return model_ft\n",
        "\n",
        "#Define model/ neural network class\n",
        "# class ImageClassifier(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(ImageClassifer, self).__init__()\n",
        "#         self.flatten = nn.Flatten()\n",
        "#         self.model_stack = nn.Sequential(\n",
        "#             nn.Linear(),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(0.2),\n",
        "#             nn.Linear(),\n",
        "#             nn.LogSoftmax(dim=1)\n",
        "#         )\n",
        "               \n",
        "#     def forward(self, x):\n",
        "#         x = self.flatten(x)\n",
        "#         logits = self.model_stack(x)\n",
        "#         return logits\n",
        "class TinyVGG(nn.Module):\n",
        "    \"\"\"Creates the TinyVGG architecture.\n",
        "\n",
        "    Replicates the TinyVGG architecture from the CNN explainer website in PyTorch.\n",
        "    See the original architecture here: https://poloclub.github.io/cnn-explainer/\n",
        "\n",
        "    Args:\n",
        "    input_shape: An integer indicating number of input channels.\n",
        "    hidden_units: An integer indicating number of hidden units between layers.\n",
        "    output_shape: An integer indicating number of output units.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
        "        super().__init__()\n",
        "        self.conv_block_1 = nn.Sequential(\n",
        "          nn.Conv2d(in_channels=input_shape, \n",
        "                    out_channels=hidden_units, \n",
        "                    kernel_size=3, \n",
        "                    stride=1, \n",
        "                    padding=0),  \n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(in_channels=hidden_units, \n",
        "                    out_channels=hidden_units,\n",
        "                    kernel_size=3,\n",
        "                    stride=1,\n",
        "                    padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(kernel_size=2,\n",
        "                        stride=2)\n",
        "        )\n",
        "        self.conv_block_2 = nn.Sequential(\n",
        "          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "          nn.Flatten(),\n",
        "          # Where did this in_features shape come from? \n",
        "          # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
        "          nn.Linear(in_features=hidden_units*13*13,\n",
        "                    out_features=output_shape)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.conv_block_1(x)\n",
        "        x = self.conv_block_2(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "        # return self.classifier(self.block_2(self.block_1(x))) # <- leverage the benefits of operator fusion"
      ],
      "metadata": {
        "id": "Lh4c_SfnLBm3"
      },
      "execution_count": 125,
      "outputs": []
    }
  ]
}